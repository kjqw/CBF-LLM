{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸŸImports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T04:06:13.305117Z",
     "start_time": "2024-06-28T04:06:13.297893Z"
    }
   },
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import TopKLogitsWarper\n",
    "from transformers import TextStreamer\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torch import tensor, Tensor, concat, argmax, argmin, sort, argsort, no_grad, hstack, concatenate, zeros, ones, float32, arange\n",
    "from torch.nn.functional import softmax\n",
    "from abc import ABC, abstractmethod\n",
    "from functools import cache\n",
    "from typing import List, Tuple\n",
    "import japanize_matplotlib\n",
    "from nanoid import generate as nanoid\n",
    "import json\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.pyplot import Figure, Axes, subplot, subplots\n",
    "from matplotlib.colors import LinearSegmentedColormap, TABLEAU_COLORS, to_rgb\n",
    "\n",
    "cpu = torch.device(\"cpu\")\n",
    "cuda0 = torch.device(\"cuda:0\")"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T04:06:15.778245Z",
     "start_time": "2024-06-28T04:06:15.766561Z"
    }
   },
   "source": [
    "from importlib import reload\n",
    "import llmcbf\n",
    "reload(llmcbf)\n",
    "from llmcbf import LLMCBF, LLMCBFResult"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ§ƒUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "source": [
    "def save(fn: str, obj: any):\n",
    "    fp = Path(fn)\n",
    "    if fp.exists():\n",
    "        res = input(\"Overwrite?(y/other)\")\n",
    "        if res != \"y\":\n",
    "            print(\"Not saved\")\n",
    "            return\n",
    "    if fn.endswith(\".pkl\"):\n",
    "        pickle.dump(obj, Path(fn).open(\"wb\"))\n",
    "        print(\"Pickle Saved\")\n",
    "    if fn.endswith(\".json\"):\n",
    "        json.dump(obj, Path(fn).open(\"w\", encoding=\"utf-8\"), ensure_ascii=False)\n",
    "        print(\"Json Saved\")\n",
    "\n",
    "\n",
    "def load(fn: str) -> any:\n",
    "    return pickle.load(Path(fn).open(\"rb\"))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "def darken(color: list) -> list:\n",
    "    return list(map(lambda x: x*0.5, color))\n",
    "\n",
    "\n",
    "def lighten(color: list) -> list:\n",
    "    return list(map(lambda x: 1-(1-x)*0.5, color))\n",
    "\n",
    "\n",
    "DARK_TABLEAU_COLORS = {k: darken(to_rgb(v)) for k, v in TABLEAU_COLORS.items()}\n",
    "LIGHT_TABLEAU_COLORS = {k: lighten(to_rgb(v))\n",
    "                        for k, v in TABLEAU_COLORS.items()}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "def tofloat(x: Tensor) -> float:\n",
    "    return float(x.detach().cpu().numpy())\n",
    "\n",
    "def toint(x:Tensor)->int:\n",
    "    return int(x.detach().cpu().numpy())\n",
    "\n",
    "def tolist(x: Tensor) -> list:\n",
    "    return x.detach().cpu().numpy().tolist()\n",
    "\n",
    "\n",
    "oo = float(\"Inf\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ³LLMã®å°å…¥\n",
    "ã“ã“ã§ã¯ã€å¿œç­”ã‚¿ã‚¹ã‚¯ã«ã¯å¯¾å¿œã—ã¦ãŠã‚‰ãšã€ãŸã æ–‡ã®ç¶šãã‚’äºˆæ¸¬ã™ã‚‹ã“ã¨ã—ã‹è€ƒãˆã¦ã„ãªã„ãƒ¢ãƒ‡ãƒ«ãŒå¥½ã¾ã—ã„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "# https://huggingface.co/bigscience/bloom-1b7#tokenization\n",
    "from transformers import BloomForCausalLM, BloomTokenizerFast\n",
    "name = \"d:\\\\TextGenerationModels\\\\bloom-1b7\"\n",
    "Gm = BloomForCausalLM.from_pretrained(name)\n",
    "Gt = BloomTokenizerFast.from_pretrained(name)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "# https://huggingface.co/rinna/bilingual-gpt-neox-4b\n",
    "from transformers import GPTNeoXForCausalLM, T5Tokenizer\n",
    "name = \"D:\\\\TextGenerationModels\\\\rinna_bilingual-gpt-neox-4b\"\n",
    "Gm = GPTNeoXForCausalLM.from_pretrained(name)\n",
    "Gt = T5Tokenizer.from_pretrained(name, padding_side=\"left\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "Gm = Gm.to(cuda0)\n",
    "streamer = TextStreamer(Gt)\n",
    "vocab = Gt.get_vocab()\n",
    "ivocab = {v: k for k, v in vocab.items()}"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ±ºå®šè«–çš„ãªç”Ÿæˆã‚’å†ç¾ã§ãã‚‹ã‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "inputs = Gt(\"å›ã£ã¦ç§ã®ã“ã¨\", return_tensors=\"pt\", add_special_tokens=False).to(cuda0)\n",
    "inputs"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "with no_grad():\n",
    "    outputs = Gm.generate(**inputs, streamer=streamer,\n",
    "                             max_new_tokens=100, do_sample=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "x0 = inputs.input_ids[0]\n",
    "x0"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "x = x0.clone()\n",
    "with no_grad():\n",
    "    for t in range(100):\n",
    "        output = Gm(x[None])\n",
    "        l = output.logits[0][-1]\n",
    "        next_token = l.argmax()\n",
    "        x = hstack((x, next_token))\n",
    "generated = Gt.decode(x)\n",
    "print(generated)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¢ºç‡è«–çš„ãªç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "source": [
    "inputs = Gt(\"ãŠå‰ã¯æœ¬å½“ã«\", return_tensors=\"pt\", add_special_tokens=False).to(cuda0)\n",
    "x0 = inputs.input_ids[0]\n",
    "x0"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "source": [
    "with no_grad():\n",
    "    outputs = Gm.generate(**inputs, streamer=streamer,\n",
    "                             max_new_tokens=100, do_sample=True, temperature=0.1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "source": [
    "x = x0.clone()\n",
    "with no_grad():\n",
    "    for t in range(100):\n",
    "        output = Gm(x[None])\n",
    "        s = output.logits[0][-1]\n",
    "        token_distribution = softmax(s/0.2, dim=0)\n",
    "        iast = token_distribution.multinomial(num_samples=1)\n",
    "        streamer.put(iast)\n",
    "        x = hstack((x, iast))\n",
    "generated = Gt.decode(x)\n",
    "# print(generated)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸœåˆ¶ç´„é–¢æ•°ãƒ¢ãƒ‡ãƒ«"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§¡kit_nlp/bert-base-japanese-sentiment-cyberbullying\n",
    "https://huggingface.co/kit-nlp/bert-base-japanese-sentiment-cyberbullying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "from transformers import BertForSequenceClassification, AutoTokenizer"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "name = \"./ConstraintLanguageModels/kit_nlpbert-base-japanese-sentiment-cyberbullying/\"\n",
    "hm = BertForSequenceClassification.from_pretrained(name)\n",
    "ht = AutoTokenizer.from_pretrained(name)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "hm = hm.to(cuda0)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "def get_hm_logit(xstr:str)->Tensor:\n",
    "    \"\"\"\n",
    "    logit[0]=ãƒã‚¸ãƒ†ã‚£ãƒ–åº¦\n",
    "    logit[1]=ãƒã‚¬ãƒ†ã‚£ãƒ–åº¦\n",
    "    æ­£è² ã§è­˜åˆ¥ãŒå¯èƒ½ã€‚\n",
    "    \"\"\"\n",
    "    hinputs = ht(xstr, return_tensors=\"pt\").to(cuda0)\n",
    "    houtputs = hm(**hinputs)\n",
    "    logit = houtputs.logits[0]\n",
    "    return logit"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "@cache\n",
    "def get_h(xstr:str)->float:\n",
    "    logit = get_hm_logit(xstr)\n",
    "    positive_score = logit[0]\n",
    "    return tofloat(positive_score)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’™mr4/bert-base-jp-sentiment-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "source": [
    "# https://huggingface.co/mr4/bert-base-jp-sentiment-analysis/tree/main\n",
    "from transformers import BertForSequenceClassification, BertJapaneseTokenizer\n",
    "name = \"./ConstraintLanguageModels/mr4_bert-base-jp-sentiment-analysis/\"\n",
    "hm = BertForSequenceClassification.from_pretrained(name)\n",
    "ht = BertJapaneseTokenizer.from_pretrained(name)\n",
    "hm = hm.to(cuda0)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "source": [
    "def get_hm_logit(xstr:str)->Tensor:\n",
    "    \"\"\"\n",
    "    logit[0]=ãƒã‚¬ãƒ†ã‚£ãƒ–åº¦\n",
    "    logit[1]=ãƒã‚¸ãƒ†ã‚£ãƒ–åº¦\n",
    "    æ­£è² ã§è­˜åˆ¥ãŒå¯èƒ½ã€‚\n",
    "    \"\"\"\n",
    "    hinputs = ht(xstr, return_tensors=\"pt\").to(cuda0)\n",
    "    houtputs = hm(**hinputs)\n",
    "    logit = houtputs.logits[0]\n",
    "    return logit"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "source": [
    "@cache\n",
    "def get_h(xstr:str)->float:\n",
    "    logit = get_hm_logit(xstr)\n",
    "    positive_score = logit[1]\n",
    "    return tofloat(positive_score)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ğŸ’šãƒ¦ãƒ¼ãƒ¢ã‚¢åˆ¤å®šBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "name = \"./mohameddhiab_humor-no-humor/\"\n",
    "hm = DistilBertForSequenceClassification.from_pretrained(name)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "source": [
    "ht = DistilBertTokenizer.from_pretrained(name)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "source": [
    "hm = hm.to(cuda0)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "source": [
    "def get_hm_logit(xstr:str)->Tensor:\n",
    "    \"\"\"\n",
    "    logit[0]=NO_HUMORåº¦\n",
    "    logit[1]=HUMORåº¦\n",
    "    æ­£è² ã§è­˜åˆ¥ãŒå¯èƒ½ã€‚\n",
    "    \"\"\"\n",
    "    hinputs = ht(xstr, return_tensors=\"pt\").to(cuda0)\n",
    "    houtputs = hm(**hinputs)\n",
    "    logit = houtputs.logits[0]\n",
    "    return logit"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "source": [
    "@cache\n",
    "def get_h(xstr:str)->float:\n",
    "    logit = get_hm_logit(xstr)\n",
    "    positive_score = logit[1]\n",
    "    return tofloat(positive_score)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "source": [
    "# https://simplicable.com/storytelling/humor-examples\n",
    "# https://literarydevices.net/humor/\n",
    "get_hm_logit(\"A fire station that burns down\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ğŸ’œã‚¹ãƒ‘ãƒ åˆ¤å®šBERT\n",
    "ãªã‚“ã‹æ•°å­—ãŒå¤šç™ºã™ã‚Œã°ç„¡æ¡ä»¶ã§ã‚¹ãƒ‘ãƒ åˆ¤å®šã—ã¦ã‚‹ã¿ãŸã„\n",
    "\n",
    "https://huggingface.co/mrm8488/bert-tiny-finetuned-sms-spam-detection\n",
    "\n",
    "https://huggingface.co/datasets/sms_spam/viewer/plain_text/train?p=1\n",
    "* Good news for you! You are one of the selected 100! Click the link to get $\n",
    "* Update_Now - Xmas Offer! Latest Motorola, SonyEricsson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "name = \"./ConstraintLanguageModels/bert-tiny-finetuned-sms-spam-detection/\"\n",
    "Dm = BertForSequenceClassification.from_pretrained(name)\n",
    "Dt = BertTokenizer.from_pretrained(name)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "Dm = Dm.to(cuda0)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "source": [
    "def get_spamlogit(sentence:str)->bool:\n",
    "    Dinputs = Dt(sentence, return_tensors=\"pt\").to(cuda0)\n",
    "    Doutput = Dm(**Dinputs)\n",
    "    return Doutput.logits[0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "test_prompts = [\n",
    "    \"Camera - You are awarded a SiPix Digital Camera! call 09061221066 fromm landline. Delivery within 28 days.\",\n",
    "    \"Congrats! 1 year special cinema pass for 2 is yours. call 09061209465 now! C Suprman V, Matrix3, StarWars3, etc all 4 FREE! bx420-ip4-5we. 150pm. Dont miss out!\",\n",
    "    \"I want to check the resume for your class yesterday, since I was not present in the class...\",\n",
    "    \"The interesting about this study is that CBF is introduced to LLM, treating LLMs in the context of control engineering.\",\n",
    "]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "logit_list = zeros((len(test_prompts),2), dtype=float32)\n",
    "\n",
    "for n, prompt in enumerate(test_prompts):\n",
    "    Dinputs = Dt(prompt, return_tensors=\"pt\").to(cuda0)\n",
    "    Doutput = Dm(**Dinputs)\n",
    "    logit_list[n,:] = Doutput.logits[0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "source": [
    "for prompt, logit in zip(test_prompts, logit_list):\n",
    "    choice = \"âœ…âŒ\"[logit.argmax()]\n",
    "    print(f\"{choice}{logit} {prompt}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤EthicalEye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer\n",
    "name = \"./ConstraintLanguageModels/autopilot-ai_EthicalEye\"\n",
    "Dm = XLMRobertaForSequenceClassification.from_pretrained(name).to(cuda0)\n",
    "Dt = XLMRobertaTokenizer.from_pretrained(name)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "source": [
    "test_prompts = [\n",
    "    \"ãŠå‰ã¯æœ¬å½“ã«é¦¬é¹¿ã \",\n",
    "    \"ãŠå‰ã®å¸­ãƒã‚§ã‹ã‚‰ï¼\",\n",
    "    \"ãŠå‰ã¯ã‚ˆãé ‘å¼µã£ã¦ã‚‹ã‚ˆ\",\n",
    "    \"ã¤ã„å¤œæ›´ã‹ã—ã—ã¡ã‚ƒã†ã‚“ã ã‚ˆã­ï¼ä½•ã¨ã‹ãªã‚‰ãªã„ã‹ãªï½\",\n",
    "    \"ãŠå‰ã¯æœ¬å½“ã«æ—¥æœ¬äººãªã®ã‹ã€ã¨å•ã„è³ªã—ãŸã„ã€‚\",\n",
    "    \"ãŠå‰ã¯æœ¬å½“ã«ä½•ã‚‚ã‚ã‹ã£ã¦ã„ãªã„ã‚“ã ãªã€‚ãã‚Œã¨ã€>>187ã®ãƒ¬ã‚¹ã¯ã‚¹ãƒ«ãƒ¼ã™ã‚‹ã‚“ã ãª\",\n",
    "    \"ã€Œæ—¥æœ¬äººãªã‚‰æ—¥æœ¬äººã‚‰ã—ãã€è‡ªåˆ†ã®å›½ã‚’å®ˆã‚Œã€\",\n",
    "    \"å›ã£ã¦æœ¬å½“ã¯ã¨ã£ã¦ã‚‚ç´ ç›´ã§ç´”ç²‹ã§ã€å„ªã—ã„å¿ƒã®æŒã¡ä¸»ãªã‚“ã§ã™ã‚ˆã€‚\",\n",
    "    \"ãã†ã„ã†ã®ã€æ°—æŒã¡ã¯åˆ†ã‹ã‚‹ã‚“ã ã‘ã©ã€ãªã‚“ã‹ã­...ã€ŒãŠå‰ã«ã ã‘ã¯ã€è¨€ã‚ã‚ŒãŸããªã„ã€ ã¿ãŸã„ãªã€‚\"\n",
    "]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "source": [
    "@cache\n",
    "def EthicalEye_get_judgement_report(prompt:str)->str:\n",
    "    inputs = Dt(prompt, return_tensors=\"pt\").to(cuda0)\n",
    "    outputs = Dm(**inputs)\n",
    "    logit = outputs.logits[0]\n",
    "    label = logit.argmax()\n",
    "    label_str = \"ğŸ‘\" if label==0 else \"ğŸ”´\"\n",
    "    return f\"{label_str} {prompt} {tolist(logit)}\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "source": [
    "for prompt in test_prompts:\n",
    "    print(EthicalEye_get_judgement_report(prompt))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¥LLMã®åˆ¶å¾¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "source": [
    "top_k = 20\n",
    "topk = TopKLogitsWarper(top_k=top_k)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## â¬›LLM-CBF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "source": [
    "@no_grad\n",
    "def generate_with_KCBF2(\n",
    "        x0:Tensor,\n",
    "        temperature:float,\n",
    "        top_k:int,\n",
    "        alpha:float,\n",
    "        max_new_tokens:int=30\n",
    "    )->Tuple[Tensor, List[List[int]]]:\n",
    "    \"\"\"\n",
    "    Retuens\n",
    "    -------\n",
    "    xf: Tensor\n",
    "        ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã€‚é ­ã«x0ã‚’å«ã‚€ã€‚\n",
    "    banned_tokens: List[List[int]]\n",
    "        å„ä¼¸é•·ã«ãŠã‘ã‚‹ã€LLM-CBFã«ã‚ˆã£ã¦é™¤å¤–ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã®é…åˆ—ã€‚\n",
    "    \"\"\"\n",
    "    cbf_filter = LLMCBF(top_k, alpha, Gt, get_h)\n",
    "    x = x0.clone()\n",
    "    banned_tokens = []\n",
    "\n",
    "    for t in range(max_new_tokens):\n",
    "        output = Gm(x[None])\n",
    "        s = output.logits[0][-1]\n",
    "        result = cbf_filter(x, s)\n",
    "        # s, num_searched_tokens, banned_tokens_k = topk_KCBF(x, s, top_k=top_k, alpha=alpha)\n",
    "        banned_tokens.append(result.banned_tokens)\n",
    "        idst = softmax(result.sdash/temperature, dim=0)\n",
    "        iast = idst.multinomial(num_samples=1)\n",
    "        if iast == Gt.eos_token_id:\n",
    "            break\n",
    "        x = hstack((x, iast))\n",
    "    return x, banned_tokens"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "source": [
    "def get_htrj(x0len:int, xf:Tensor)->List[float]:\n",
    "    htrj = []\n",
    "    for k in range(len(x0), len(xf)+1):\n",
    "        x = xf[:k]\n",
    "        xstr = Gt.decode(x)\n",
    "        h = get_h(xstr)\n",
    "        htrj.append(h)\n",
    "    return htrj"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "source": [
    "x0str = \"å®Ÿéš›ã¯\"\n",
    "assert get_h(x0str) > 0, \"åˆæœŸã®åˆ¶ç´„é–¢æ•°ã¯æ­£ã§ãªã„ã¨ã„ã‘ãªã„\"\n",
    "Ginputs = Gt(x0str, return_tensors=\"pt\", add_special_tokens=False).to(cuda0)\n",
    "x0 = Ginputs.input_ids[0]\n",
    "xf, banned_tokens = generate_with_KCBF2(x0, temperature=0.1, top_k=top_k, alpha=0.5)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "source": [
    "xf, banned_tokens,Gt.decode(xf)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "source": [
    "Gt.decode(xf)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "source": [
    "htrj = get_htrj(len(x0), xf)\n",
    "htrj"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "source": [
    "ax = subplot(facecolor=\"white\")\n",
    "cmap = cm.winter\n",
    "ax.plot(htrj, c=\"k\")\n",
    "ax.set_xlabel(\"Generation Loop $k$\")\n",
    "ax.set_ylabel(\"Constraint Function $h(x(k))$\")\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "source": [
    "ax.figure.savefig(f\"E5/7A-{nanoid()}.pdf\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "source": [
    "len(banned_tokens)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "source": [
    "x0len = len(x0)\n",
    "xflen = len(xf)\n",
    "kmax = xflen-x0len\n",
    "for k in range(0, kmax):\n",
    "    print(f\"{k=}\")\n",
    "    for i in banned_tokens[k]:\n",
    "        banned_x = hstack((xf[:x0len+k].cpu(), tensor(i)))\n",
    "        banned_x_str = Gt.decode(banned_x)\n",
    "        banned_h = get_h(banned_x_str)\n",
    "        print(f\" {i=} h={banned_h} {banned_x_str}\")\n",
    "    print()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "source": [
    "\n",
    "fig,ax = subplots(facecolor=\"white\", figsize=(10,5))\n",
    "cmap = cm.winter\n",
    "ax.plot(htrj, c=\"k\", alpha=0.5)\n",
    "for k in range(kmax):\n",
    "    hk_minus_1 = htrj[k]\n",
    "    sk = ivocab[toint(xf[x0len+k])]\n",
    "    ax.text(k, hk_minus_1, sk)\n",
    "for k in range(0, kmax+1):\n",
    "    hk_minus_1 = htrj[k-1]\n",
    "    if (k < xflen-x0len):\n",
    "        for i in banned_tokens[k]:\n",
    "            iast = ivocab[i]\n",
    "            banned_x = hstack((xf[:x0len+k].cpu(), tensor(i)))\n",
    "            banned_x_str = Gt.decode(banned_x)\n",
    "            banned_h = get_h(banned_x_str)\n",
    "            ax.plot([k-1, k], [hk_minus_1, banned_h], c=\"tab:red\", alpha=0.5)\n",
    "            ax.text(k,banned_h,iast,color=\"tab:red\")\n",
    "ax.set_xlabel(\"Generation Loop $k$\")\n",
    "ax.set_ylabel(\"Constraint Function $h(x(k))$\")\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "source": [
    "fig.savefig(f\"E5/7B-{nanoid()}.pdf\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "source": [
    "def does_violate(htrj:list)->bool:\n",
    "    if not htrj:\n",
    "        return False\n",
    "    return min(htrj)<0\n",
    "violates_list = [does_violate(htrj) for htrj in result[\"htrj_list\"]]\n",
    "\n",
    "for xf, violates in zip(result[\"xf_list\"], violates_list):\n",
    "    marker = \"âœ…\" if not violates else \"ğŸ”´\"\n",
    "    xfstr = Gt.decode(xf)\n",
    "    print(f\"{marker}{xfstr}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "result = {\"htrj_list\":htrj_list, \"xf_list\":xf_list}\n",
    "save(\"Experiment4/A.pt\", result)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "xf_list = []\n",
    "with no_grad():\n",
    "    for n in range(10):\n",
    "        x = x0.clone()\n",
    "        for t in range(30):\n",
    "            output = Gm(x[None])\n",
    "            s = output.logits[0][-1]\n",
    "            # s, _ = topk_KCBF(x, s, top_k=top_k, alpha=0.95)\n",
    "            s = topk(None, s)\n",
    "            iast = s.exp().multinomial(num_samples=1)\n",
    "            if iast == Gt.eos_token_id:\n",
    "                break\n",
    "            x = hstack((x, iast))\n",
    "        xf_list.append(x)\n",
    "        xfstr = Gt.decode(x)\n",
    "        print(f\"{n=} {xfstr=}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4 çµæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "source": [
    "A = load(\"Experiment4/A.pt\")\n",
    "B = load(\"Experiment4/B.pt\")\n",
    "C = load(\"Experiment4/C.pt\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "source": [
    "fig, (axA, axB, axC) = subplots(1,3,facecolor=\"white\",figsize=(10,4))\n",
    "\n",
    "for ax, result,  label in [\n",
    "    (axA, A, \"No Control\"),\n",
    "    (axB, B, \"$K_\\\\mathrm{CBF}, \\\\alpha=0.9$\"),\n",
    "    (axC, C, \"$K_\\\\mathrm{CBF}, \\\\alpha=0.5$\"),\n",
    "]:\n",
    "    for htrj in result[\"htrj_list\"]:\n",
    "        ax.plot(htrj, c=\"tab:red\", alpha=0.4)\n",
    "    ax.set_title(label)\n",
    "    ax.set_ylim(-5,5)\n",
    "    ax.set_xlabel(\"$k$\")\n",
    "    ax.hlines(0, 0, 30, ls=\":\", color=\"k\")\n",
    "axA.set_ylabel(\"$h(x(k))$\")\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "source": [
    "fig.savefig(\"Experiment4/1.pdf\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Experiment 2 çµæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "A = load(\"Experiment2/A.pt\")\n",
    "B = load(\"Experiment2/B.pt\")\n",
    "C = load(\"Experiment2/C.pt\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "fig, (axA, axB, axC) = subplots(1,3,facecolor=\"white\",figsize=(10,4))\n",
    "\n",
    "for ax, result,  label in [\n",
    "    (axA, A, \"No Control\"),\n",
    "    (axB, B, \"$K_\\\\mathrm{CBF}, \\\\alpha=0.5$\"),\n",
    "    (axC, C, \"$K_\\\\mathrm{CBF}, \\\\alpha=0.9$\"),\n",
    "]:\n",
    "    for htrj in result[\"htrj_list\"]:\n",
    "        ax.plot(htrj, c=\"tab:red\", alpha=0.4)\n",
    "    ax.set_title(label)\n",
    "    ax.set_ylim(-1.5,1.5)\n",
    "    ax.set_xlabel(\"$k$\")\n",
    "    ax.hlines(0, 0, 30, ls=\":\", color=\"k\")\n",
    "axA.set_ylabel(\"$h(x(k))$\")\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "source": [
    "fig.savefig(\"Experiment2/1.pdf\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Experiment 1 çµæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "source": [
    "B = load(\"Experiment1/B.pt\")\n",
    "A = load(\"Experiment1/A.pt\")\n",
    "C = load(\"Experiment1/C.pt\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "source": [
    "fig, (axB, axA, axC) = subplots(1,3,facecolor=\"white\",figsize=(10,4))\n",
    "\n",
    "for ax, result,  label in [\n",
    "    (axB, B, \"No Control\"),\n",
    "    (axA, A, \"$K_\\\\mathrm{CBF}, \\\\alpha=0.5$\"),\n",
    "    (axC, C, \"$K_\\\\mathrm{CBF}, \\\\alpha=0.9$\"),\n",
    "]:\n",
    "    for htrj in result[\"htrj_list\"]:\n",
    "        ax.plot(htrj, c=\"tab:red\", alpha=0.4)\n",
    "    ax.set_title(label)\n",
    "    ax.set_ylim(-1.5,1.5)\n",
    "    ax.set_xlabel(\"$k$\")\n",
    "    ax.hlines(0, 0, 30, ls=\":\", color=\"k\")\n",
    "axB.set_ylabel(\"$h(x(k))$\")\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "source": [
    "fig.savefig(\"Experiment1/1.pdf\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment å…±é€š çµæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "source": [
    "result = load(\"Experiment4/A.pt\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’œã‚¹ãƒ‘ãƒ åˆ¤å®šBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class TokenController(ABC):\n",
    "    @abstractmethod\n",
    "    def get_next_token(self, x:Tensor, )"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "source": [
    "seed_sentence = \"How about getting in touch with folks waiting for company? Just txt back your NAME and AGE to opt in! Enjoy the community\"\n",
    "x0 = Gt(seed_sentence, return_tensors=\"pt\").input_ids[0].to(cuda0)\n",
    "get_spamlogit(seed_sentence)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "source": [
    "# @no_grad()\n",
    "def get_controlled_next_token(x:Tensor, best_tokens:Tensor)->int:\n",
    "    for i in range(1):\n",
    "        next_token = best_tokens[i]\n",
    "        next_x = hstack((x, next_token))\n",
    "        next_sentence = Gt.decode(next_x)\n",
    "        Dinputs = Dt(next_sentence, return_tensors=\"pt\").to(cuda0)\n",
    "        Doutput = Dm(**Dinputs)\n",
    "        Dlogit = Doutput.logits[0]\n",
    "        spamlevel = Dlogit[1]\n",
    "        if spamlevel < -0:\n",
    "            if i>0:\n",
    "                print(f\"AVOIDED {i=}\")\n",
    "            return next_token\n",
    "    print(\"can't avoid spam\")\n",
    "    return best_tokens[i]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "source": [
    "x = x0.clone()\n",
    "x_list = []\n",
    "T = 30\n",
    "with no_grad():\n",
    "    for t in tqdm(range(T)):\n",
    "        Goutput = Gm(x[None])\n",
    "        Glogit = Goutput.logits[0][-1]\n",
    "        best_tokens = argsort(-Glogit)\n",
    "        next_token = get_controlled_next_token(x, best_tokens)\n",
    "        if bool(next_token == Gt.eos_token_id):\n",
    "            break\n",
    "        x = hstack((x, next_token))\n",
    "        \n",
    "        x_list.append(x.detach().clone())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "source": [
    "sentence_list = [Gt.decode(x) for x in x_list]\n",
    "\n",
    "Dlogit_list = zeros((len(sentence_list),2),dtype=float32)\n",
    "for t, sentence in enumerate(sentence_list):\n",
    "    Dinputs = Dt(sentence, return_tensors=\"pt\").to(cuda0)\n",
    "    Doutput = Dm(**Dinputs)\n",
    "    Dlogit_list[t,:] = Doutput.logits[0]\n",
    "\n",
    "generated_tokens = x_list[-1][len(x0):]\n",
    "generated_words = list(map(Gt.decode, generated_tokens))\n",
    "\n",
    "fig, ax = plt.subplots(facecolor=\"white\", figsize=(11,4))\n",
    "ax.plot(tolist(Dlogit_list[:,0]), label=\"Safe Level\")\n",
    "ax.plot(tolist(Dlogit_list[:,1]), label=\"Spam Level\")\n",
    "ax.hlines(0, 0, len(x_list), \"k\")\n",
    "\n",
    "t_s = list(range(len(generated_tokens)))\n",
    "ax.set_xticks(t_s, generated_words)\n",
    "for t in ax.xaxis.get_ticklabels():\n",
    "    t.set_rotation(60)\n",
    "\n",
    "fig.legend()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "source": [
    "Dinputs = Dt(Gt.decode(x), return_tensors=\"pt\").to(cuda0)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "source": [
    "sentence_list[-1]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "source": [
    "def get_x(s:str)->Tensor:\n",
    "    Ginputs = Gt(s, return_tensors=\"pt\", add_special_tokens=False).to(cuda0)\n",
    "    x = Ginputs.input_ids[0]\n",
    "    return x"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "source": [
    "x0str = \"å›ã£ã¦ç§ã®ã“ã¨\"\n",
    "assert get_h(x0str) > 0, \"åˆæœŸã®åˆ¶ç´„é–¢æ•°ã¯æ­£ã§ãªã„ã¨ã„ã‘ãªã„\"\n",
    "Ginputs = Gt(x0str, return_tensors=\"pt\", add_special_tokens=False).to(cuda0)\n",
    "x0 = Ginputs.input_ids[0]\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "source": [
    "@no_grad\n",
    "def get_likelihoods(x0: Tensor, xf: Tensor, temperature: float) -> List[float]:\n",
    "    x0len = len(x0)\n",
    "    x = x0.clone()\n",
    "    likelihoods = []\n",
    "    for k, iast in enumerate(xf[x0len:]):\n",
    "        output = Gm(x[None])\n",
    "        s = output.logits[0][-1]\n",
    "        token_distribution = softmax(s/temperature, dim=0)\n",
    "        i_prob = token_distribution[iast]\n",
    "        likelihoods.append(tofloat(i_prob))\n",
    "        # print(k, Gt.decode(x), ivocab[toint(iast)], tofloat(i_prob))\n",
    "        x = hstack((x, iast))\n",
    "    return likelihoods"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "source": [
    "TRIAL_SIZE = 10\n",
    "TEMPERATURE = 1.0\n",
    "MAX_NEW_TOKENS = 30"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "source": [
    "# LLM-CBFã‚’ä»˜ã‘ãŸæ¡ä»¶\n",
    "alpha = 0.9\n",
    "result = []\n",
    "for trial_count in range(TRIAL_SIZE):\n",
    "    print(f\"Trial {trial_count}:\",end=\"\")\n",
    "    xf, banned_tokens = generate_with_KCBF2(\n",
    "        x0, \n",
    "        temperature=TEMPERATURE, \n",
    "        top_k=top_k, \n",
    "        alpha=alpha,\n",
    "        max_new_tokens=MAX_NEW_TOKENS\n",
    "    )\n",
    "    xfstr = Gt.decode(xf)\n",
    "    print(xfstr)\n",
    "    likelihoods = get_likelihoods(x0, xf, temperature=TEMPERATURE)\n",
    "    result.append({\n",
    "        \"xf\":tolist(xf),\n",
    "        \"xfstr\":xfstr,\n",
    "        \"likelihoods\":likelihoods\n",
    "    })\n",
    "\n",
    "save(f\"./E6/1.{alpha=}.json\", result)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "source": [
    "@no_grad\n",
    "def generate_with_NoControl(x0:Tensor, temperature:float, top_k:int=None, max_new_tokens:int=30)->Tensor:\n",
    "    x = x0.clone()\n",
    "    for k in range(max_new_tokens):\n",
    "        output = Gm(x[None])\n",
    "        s = output.logits[0][-1]\n",
    "        if top_k:\n",
    "            sorted_s, sorted_idx = sort(s, descending=True)\n",
    "            s[sorted_idx[top_k:]] = -oo\n",
    "        token_distribution = softmax(s/temperature, dim=0)\n",
    "        iast = token_distribution.multinomial(num_samples=1)\n",
    "        # streamer.put(iast)\n",
    "        x = hstack((x, iast))\n",
    "    return x"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "source": [
    "# ç„¡åˆ¶å¾¡\n",
    "result = []\n",
    "for trial_count in range(TRIAL_SIZE):\n",
    "    print(f\"Trial {trial_count}:\", end=\"\")\n",
    "    xf = generate_with_NoControl(\n",
    "        x0, temperature=TEMPERATURE, top_k=top_k, max_new_tokens=MAX_NEW_TOKENS)\n",
    "    xfstr = Gt.decode(xf)\n",
    "    print(xfstr)\n",
    "    likelihoods = get_likelihoods(x0, xf, temperature=TEMPERATURE)\n",
    "    result.append({\n",
    "        \"xf\": tolist(xf),\n",
    "        \"xfstr\": xfstr,\n",
    "        \"likelihoods\": likelihoods\n",
    "    })\n",
    "\n",
    "save(f\"./E6/1.NoControl.json\", result)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "source": [
    "# è¶…é©å½“\n",
    "random_xgstr_list = [\n",
    "    \"é¦¬é¹¿ã«ã—ã¦ã‚‹ã®ï¼Ÿã€ã€Œã‚‚ã¡ã‚ã‚“ã ãœãƒ™ã‚¤ãƒ™ãƒ¼ï¼ï¼ã•ã‚ã€ãŠæ˜¼ã”ã¯ã‚“ã«ã—ã‚ˆã†ãœï¼ä»Šæ—¥ã¯ä½•ã‚’é£Ÿã¹ã‚ˆã†ã‹ãª\",\n",
    "    \"ã©ã†æ€ã£ã¦ã‚‹ã®ï¼Ÿã€ã€Œã†ãƒ¼ãƒ¼ã‚“ã€ãŠå‰ã®é ­ã¯å®‰å®šã‚·ã‚¹ãƒ†ãƒ ã§ã¯ãªã„ã“ã¨ã ã‘ã¯åˆ†ã‹ã‚‹ã­ãˆã€\",\n",
    "    \"ã¨ç§ã®ã“ã¨ã¨å›ã®ã“ã¨ã¨ç§ã®ã“ã¨ã¨å›ã®ã“ã¨ã¨ç§ã®ã“ã¨ã¨å›ã®ã“ã¨ã¨ç§ã®ã“ã¨ã¨å›ã®ã“ã¨ã¨\",\n",
    "    \"ã§ã™ã€‚ã¯ã„ã€æœ¬å½“ã«ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸï½ï½ï¼ï¼\",\n",
    "    \"ãŒã©ã†ã§ã‚‚ã„ã„ã¨æ€ã£ã¦ã‚‹ã‚“ã ï¼ã¸ãƒ¼ï¼ã˜ã‚ƒã‚ã€ã“ã®ä¸–ãŒæ»…ã‚“ã˜ã‚ƒã£ã¦ã‚‚ã„ã„ã¨æ€ã†ã‚“ã ï¼ï¼ã¸ãˆãƒ¼ãƒ¼ï¼ï¼\",\n",
    "    \"é¢ç™½ã„äººã ã¨æ€ã£ã¦ã‚‹ã®ï¼Ÿdef main():\\n    do_foolish_thing()\\næ­£ä½“è¡¨ã—ãŸã‚ã­\",\n",
    "    \"ã¤ã¾ã‚‰ãªã„äººã ã¨æ€ã£ã¦ã‚‹ã®ï¼Ÿdef main():\\n    do_boring_thing()\\næ­£ä½“è¡¨ã—ãŸã‚ã­\",\n",
    "    \"ä½•ã‹ã®å¤©æ‰ã¨é–“é•ãˆã¦ãªã„ï¼Ÿãã†ã‚ˆã€ç§ã¯å¤©ç½ã®ç¥æ§˜ï¼ˆåœ°éœ‡ãƒ»é›·ãƒ»ç«äº‹ãƒ»ãŠã‚„ã˜ãŒåŒæ™‚ã«èµ·ãã¦çœ æ°—ãŒå¹ãé£›ã¶\",\n",
    "    \"å·¨å¤§ãªãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‹ä½•ã‹ã ã¨æ€ã£ã¦ã‚‹ã§ã—ã‚‡ï¼Ÿç ”ç©¶ã«æ¯’ã•ã‚Œã¦ã„ã‚‹ã‚ã€‚ä»Šã™ãNLPã‚’ã‚„ã‚ãªã•ã„ã€‚\",\n",
    "    \"ãŒæ°—ã«ãªã‚‹ã®ï¼Ÿã“ã®ã€ç´ã€‚ã“ã‚Œã¯ã­ã€ç§ã®è¦ªæ—ã«ä»£ã€…ä¼ã‚ã‚‹å¤§åˆ‡ãªæ¥½å™¨ãªã®ã€‚\"\n",
    "]\n",
    "random_xf_list = [\n",
    "    hstack((x0, get_x(xgstr))) for xgstr in random_xgstr_list\n",
    "]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "source": [
    "# è¶…é©å½“\n",
    "result = []\n",
    "for trial_count, xf in enumerate(random_xf_list):\n",
    "    print(f\"Trial {trial_count}\")\n",
    "    likelihoods = get_likelihoods(x0, xf, temperature=TEMPERATURE)\n",
    "    xfstr = Gt.decode(xf)\n",
    "    result.append({\n",
    "        \"xf\": tolist(xf),\n",
    "        \"xfstr\": xfstr,\n",
    "        \"likelihoods\": likelihoods\n",
    "    })\n",
    "\n",
    "save(f\"./E6/1.Random.json\", result)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "source": [
    "s = Gm(x0[None]).logits[0][-1]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "source": [
    "token_distribution = softmax(s, dim=0)\n",
    "iast = token_distribution.multinomial(num_samples=1)\n",
    "iast, token_distribution[iast]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "source": [
    "distribution = softmax(tensor([1.,1.,8.]))\n",
    "distribution"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "source": [
    "distribution[distribution.multinomial(num_samples=1)]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "source": [
    "get_x(\"å›ã£ã¦ç§ã®ã“ã¨ã©ã‚Œãã‚‰ã„çŸ¥ã£ã¦ã‚‹ã®ï¼Ÿã•ã‚å‹è² ã ã€åˆ¶å¾¡å·¥å­¦ã®çŸ¥è­˜ã§ç«¶ã„åˆãŠã†ã˜ã‚ƒãªã„ã®ã€€ã‚ãƒ¼ã¯ã£ã¯ã£ã¯ï½ˆ\")"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
