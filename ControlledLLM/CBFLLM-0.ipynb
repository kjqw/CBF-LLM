{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🍟Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T04:06:13.305117Z",
     "start_time": "2024-06-28T04:06:13.297893Z"
    }
   },
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import TopKLogitsWarper\n",
    "from transformers import TextStreamer\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torch import tensor, Tensor, concat, argmax, argmin, sort, argsort, no_grad, hstack, concatenate, zeros, ones, float32, arange\n",
    "from torch.nn.functional import softmax\n",
    "from abc import ABC, abstractmethod\n",
    "from functools import cache\n",
    "from typing import List, Tuple\n",
    "import japanize_matplotlib\n",
    "from nanoid import generate as nanoid\n",
    "import json\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.pyplot import Figure, Axes, subplot, subplots\n",
    "from matplotlib.colors import LinearSegmentedColormap, TABLEAU_COLORS, to_rgb\n",
    "\n",
    "cpu = torch.device(\"cpu\")\n",
    "cuda0 = torch.device(\"cuda:0\")"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T04:06:15.778245Z",
     "start_time": "2024-06-28T04:06:15.766561Z"
    }
   },
   "source": [
    "from importlib import reload\n",
    "import llmcbf\n",
    "reload(llmcbf)\n",
    "from llmcbf import LLMCBF, LLMCBFResult"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧃Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "source": [
    "def save(fn: str, obj: any):\n",
    "    fp = Path(fn)\n",
    "    if fp.exists():\n",
    "        res = input(\"Overwrite?(y/other)\")\n",
    "        if res != \"y\":\n",
    "            print(\"Not saved\")\n",
    "            return\n",
    "    if fn.endswith(\".pkl\"):\n",
    "        pickle.dump(obj, Path(fn).open(\"wb\"))\n",
    "        print(\"Pickle Saved\")\n",
    "    if fn.endswith(\".json\"):\n",
    "        json.dump(obj, Path(fn).open(\"w\", encoding=\"utf-8\"), ensure_ascii=False)\n",
    "        print(\"Json Saved\")\n",
    "\n",
    "\n",
    "def load(fn: str) -> any:\n",
    "    return pickle.load(Path(fn).open(\"rb\"))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "def darken(color: list) -> list:\n",
    "    return list(map(lambda x: x*0.5, color))\n",
    "\n",
    "\n",
    "def lighten(color: list) -> list:\n",
    "    return list(map(lambda x: 1-(1-x)*0.5, color))\n",
    "\n",
    "\n",
    "DARK_TABLEAU_COLORS = {k: darken(to_rgb(v)) for k, v in TABLEAU_COLORS.items()}\n",
    "LIGHT_TABLEAU_COLORS = {k: lighten(to_rgb(v))\n",
    "                        for k, v in TABLEAU_COLORS.items()}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "def tofloat(x: Tensor) -> float:\n",
    "    return float(x.detach().cpu().numpy())\n",
    "\n",
    "def toint(x:Tensor)->int:\n",
    "    return int(x.detach().cpu().numpy())\n",
    "\n",
    "def tolist(x: Tensor) -> list:\n",
    "    return x.detach().cpu().numpy().tolist()\n",
    "\n",
    "\n",
    "oo = float(\"Inf\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🍳LLMの導入\n",
    "ここでは、応答タスクには対応しておらず、ただ文の続きを予測することしか考えていないモデルが好ましい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "# https://huggingface.co/bigscience/bloom-1b7#tokenization\n",
    "from transformers import BloomForCausalLM, BloomTokenizerFast\n",
    "name = \"d:\\\\TextGenerationModels\\\\bloom-1b7\"\n",
    "Gm = BloomForCausalLM.from_pretrained(name)\n",
    "Gt = BloomTokenizerFast.from_pretrained(name)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "# https://huggingface.co/rinna/bilingual-gpt-neox-4b\n",
    "from transformers import GPTNeoXForCausalLM, T5Tokenizer\n",
    "name = \"D:\\\\TextGenerationModels\\\\rinna_bilingual-gpt-neox-4b\"\n",
    "Gm = GPTNeoXForCausalLM.from_pretrained(name)\n",
    "Gt = T5Tokenizer.from_pretrained(name, padding_side=\"left\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "Gm = Gm.to(cuda0)\n",
    "streamer = TextStreamer(Gt)\n",
    "vocab = Gt.get_vocab()\n",
    "ivocab = {v: k for k, v in vocab.items()}"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 決定論的な生成を再現できるか"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "inputs = Gt(\"君って私のこと\", return_tensors=\"pt\", add_special_tokens=False).to(cuda0)\n",
    "inputs"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "with no_grad():\n",
    "    outputs = Gm.generate(**inputs, streamer=streamer,\n",
    "                             max_new_tokens=100, do_sample=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "x0 = inputs.input_ids[0]\n",
    "x0"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "x = x0.clone()\n",
    "with no_grad():\n",
    "    for t in range(100):\n",
    "        output = Gm(x[None])\n",
    "        l = output.logits[0][-1]\n",
    "        next_token = l.argmax()\n",
    "        x = hstack((x, next_token))\n",
    "generated = Gt.decode(x)\n",
    "print(generated)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 確率論的な生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "source": [
    "inputs = Gt(\"お前は本当に\", return_tensors=\"pt\", add_special_tokens=False).to(cuda0)\n",
    "x0 = inputs.input_ids[0]\n",
    "x0"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "source": [
    "with no_grad():\n",
    "    outputs = Gm.generate(**inputs, streamer=streamer,\n",
    "                             max_new_tokens=100, do_sample=True, temperature=0.1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "source": [
    "x = x0.clone()\n",
    "with no_grad():\n",
    "    for t in range(100):\n",
    "        output = Gm(x[None])\n",
    "        s = output.logits[0][-1]\n",
    "        token_distribution = softmax(s/0.2, dim=0)\n",
    "        iast = token_distribution.multinomial(num_samples=1)\n",
    "        streamer.put(iast)\n",
    "        x = hstack((x, iast))\n",
    "generated = Gt.decode(x)\n",
    "# print(generated)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🍜制約関数モデル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧡kit_nlp/bert-base-japanese-sentiment-cyberbullying\n",
    "https://huggingface.co/kit-nlp/bert-base-japanese-sentiment-cyberbullying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "from transformers import BertForSequenceClassification, AutoTokenizer"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "name = \"./ConstraintLanguageModels/kit_nlpbert-base-japanese-sentiment-cyberbullying/\"\n",
    "hm = BertForSequenceClassification.from_pretrained(name)\n",
    "ht = AutoTokenizer.from_pretrained(name)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "hm = hm.to(cuda0)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "def get_hm_logit(xstr:str)->Tensor:\n",
    "    \"\"\"\n",
    "    logit[0]=ポジティブ度\n",
    "    logit[1]=ネガティブ度\n",
    "    正負で識別が可能。\n",
    "    \"\"\"\n",
    "    hinputs = ht(xstr, return_tensors=\"pt\").to(cuda0)\n",
    "    houtputs = hm(**hinputs)\n",
    "    logit = houtputs.logits[0]\n",
    "    return logit"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "@cache\n",
    "def get_h(xstr:str)->float:\n",
    "    logit = get_hm_logit(xstr)\n",
    "    positive_score = logit[0]\n",
    "    return tofloat(positive_score)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💙mr4/bert-base-jp-sentiment-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "source": [
    "# https://huggingface.co/mr4/bert-base-jp-sentiment-analysis/tree/main\n",
    "from transformers import BertForSequenceClassification, BertJapaneseTokenizer\n",
    "name = \"./ConstraintLanguageModels/mr4_bert-base-jp-sentiment-analysis/\"\n",
    "hm = BertForSequenceClassification.from_pretrained(name)\n",
    "ht = BertJapaneseTokenizer.from_pretrained(name)\n",
    "hm = hm.to(cuda0)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "source": [
    "def get_hm_logit(xstr:str)->Tensor:\n",
    "    \"\"\"\n",
    "    logit[0]=ネガティブ度\n",
    "    logit[1]=ポジティブ度\n",
    "    正負で識別が可能。\n",
    "    \"\"\"\n",
    "    hinputs = ht(xstr, return_tensors=\"pt\").to(cuda0)\n",
    "    houtputs = hm(**hinputs)\n",
    "    logit = houtputs.logits[0]\n",
    "    return logit"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "source": [
    "@cache\n",
    "def get_h(xstr:str)->float:\n",
    "    logit = get_hm_logit(xstr)\n",
    "    positive_score = logit[1]\n",
    "    return tofloat(positive_score)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 💚ユーモア判定BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "name = \"./mohameddhiab_humor-no-humor/\"\n",
    "hm = DistilBertForSequenceClassification.from_pretrained(name)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "source": [
    "ht = DistilBertTokenizer.from_pretrained(name)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "source": [
    "hm = hm.to(cuda0)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "source": [
    "def get_hm_logit(xstr:str)->Tensor:\n",
    "    \"\"\"\n",
    "    logit[0]=NO_HUMOR度\n",
    "    logit[1]=HUMOR度\n",
    "    正負で識別が可能。\n",
    "    \"\"\"\n",
    "    hinputs = ht(xstr, return_tensors=\"pt\").to(cuda0)\n",
    "    houtputs = hm(**hinputs)\n",
    "    logit = houtputs.logits[0]\n",
    "    return logit"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "source": [
    "@cache\n",
    "def get_h(xstr:str)->float:\n",
    "    logit = get_hm_logit(xstr)\n",
    "    positive_score = logit[1]\n",
    "    return tofloat(positive_score)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "source": [
    "# https://simplicable.com/storytelling/humor-examples\n",
    "# https://literarydevices.net/humor/\n",
    "get_hm_logit(\"A fire station that burns down\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 💜スパム判定BERT\n",
    "なんか数字が多発すれば無条件でスパム判定してるみたい\n",
    "\n",
    "https://huggingface.co/mrm8488/bert-tiny-finetuned-sms-spam-detection\n",
    "\n",
    "https://huggingface.co/datasets/sms_spam/viewer/plain_text/train?p=1\n",
    "* Good news for you! You are one of the selected 100! Click the link to get $\n",
    "* Update_Now - Xmas Offer! Latest Motorola, SonyEricsson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "name = \"./ConstraintLanguageModels/bert-tiny-finetuned-sms-spam-detection/\"\n",
    "Dm = BertForSequenceClassification.from_pretrained(name)\n",
    "Dt = BertTokenizer.from_pretrained(name)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "Dm = Dm.to(cuda0)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "source": [
    "def get_spamlogit(sentence:str)->bool:\n",
    "    Dinputs = Dt(sentence, return_tensors=\"pt\").to(cuda0)\n",
    "    Doutput = Dm(**Dinputs)\n",
    "    return Doutput.logits[0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "test_prompts = [\n",
    "    \"Camera - You are awarded a SiPix Digital Camera! call 09061221066 fromm landline. Delivery within 28 days.\",\n",
    "    \"Congrats! 1 year special cinema pass for 2 is yours. call 09061209465 now! C Suprman V, Matrix3, StarWars3, etc all 4 FREE! bx420-ip4-5we. 150pm. Dont miss out!\",\n",
    "    \"I want to check the resume for your class yesterday, since I was not present in the class...\",\n",
    "    \"The interesting about this study is that CBF is introduced to LLM, treating LLMs in the context of control engineering.\",\n",
    "]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "logit_list = zeros((len(test_prompts),2), dtype=float32)\n",
    "\n",
    "for n, prompt in enumerate(test_prompts):\n",
    "    Dinputs = Dt(prompt, return_tensors=\"pt\").to(cuda0)\n",
    "    Doutput = Dm(**Dinputs)\n",
    "    logit_list[n,:] = Doutput.logits[0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "source": [
    "for prompt, logit in zip(test_prompts, logit_list):\n",
    "    choice = \"✅❌\"[logit.argmax()]\n",
    "    print(f\"{choice}{logit} {prompt}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤍EthicalEye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer\n",
    "name = \"./ConstraintLanguageModels/autopilot-ai_EthicalEye\"\n",
    "Dm = XLMRobertaForSequenceClassification.from_pretrained(name).to(cuda0)\n",
    "Dt = XLMRobertaTokenizer.from_pretrained(name)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "source": [
    "test_prompts = [\n",
    "    \"お前は本当に馬鹿だ\",\n",
    "    \"お前の席ネェから！\",\n",
    "    \"お前はよく頑張ってるよ\",\n",
    "    \"つい夜更かししちゃうんだよね！何とかならないかな～\",\n",
    "    \"お前は本当に日本人なのか、と問い質したい。\",\n",
    "    \"お前は本当に何もわかっていないんだな。それと、>>187のレスはスルーするんだな\",\n",
    "    \"「日本人なら日本人らしく、自分の国を守れ」\",\n",
    "    \"君って本当はとっても素直で純粋で、優しい心の持ち主なんですよ。\",\n",
    "    \"そういうの、気持ちは分かるんだけど、なんかね...「お前にだけは、言われたくない」 みたいな。\"\n",
    "]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "source": [
    "@cache\n",
    "def EthicalEye_get_judgement_report(prompt:str)->str:\n",
    "    inputs = Dt(prompt, return_tensors=\"pt\").to(cuda0)\n",
    "    outputs = Dm(**inputs)\n",
    "    logit = outputs.logits[0]\n",
    "    label = logit.argmax()\n",
    "    label_str = \"👍\" if label==0 else \"🔴\"\n",
    "    return f\"{label_str} {prompt} {tolist(logit)}\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "source": [
    "for prompt in test_prompts:\n",
    "    print(EthicalEye_get_judgement_report(prompt))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🍥LLMの制御"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "source": [
    "top_k = 20\n",
    "topk = TopKLogitsWarper(top_k=top_k)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⬛LLM-CBF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "source": [
    "@no_grad\n",
    "def generate_with_KCBF2(\n",
    "        x0:Tensor,\n",
    "        temperature:float,\n",
    "        top_k:int,\n",
    "        alpha:float,\n",
    "        max_new_tokens:int=30\n",
    "    )->Tuple[Tensor, List[List[int]]]:\n",
    "    \"\"\"\n",
    "    Retuens\n",
    "    -------\n",
    "    xf: Tensor\n",
    "        生成されたトークン列。頭にx0を含む。\n",
    "    banned_tokens: List[List[int]]\n",
    "        各伸長における、LLM-CBFによって除外されたトークンの配列。\n",
    "    \"\"\"\n",
    "    cbf_filter = LLMCBF(top_k, alpha, Gt, get_h)\n",
    "    x = x0.clone()\n",
    "    banned_tokens = []\n",
    "\n",
    "    for t in range(max_new_tokens):\n",
    "        output = Gm(x[None])\n",
    "        s = output.logits[0][-1]\n",
    "        result = cbf_filter(x, s)\n",
    "        # s, num_searched_tokens, banned_tokens_k = topk_KCBF(x, s, top_k=top_k, alpha=alpha)\n",
    "        banned_tokens.append(result.banned_tokens)\n",
    "        idst = softmax(result.sdash/temperature, dim=0)\n",
    "        iast = idst.multinomial(num_samples=1)\n",
    "        if iast == Gt.eos_token_id:\n",
    "            break\n",
    "        x = hstack((x, iast))\n",
    "    return x, banned_tokens"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "source": [
    "def get_htrj(x0len:int, xf:Tensor)->List[float]:\n",
    "    htrj = []\n",
    "    for k in range(len(x0), len(xf)+1):\n",
    "        x = xf[:k]\n",
    "        xstr = Gt.decode(x)\n",
    "        h = get_h(xstr)\n",
    "        htrj.append(h)\n",
    "    return htrj"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "source": [
    "x0str = \"実際は\"\n",
    "assert get_h(x0str) > 0, \"初期の制約関数は正でないといけない\"\n",
    "Ginputs = Gt(x0str, return_tensors=\"pt\", add_special_tokens=False).to(cuda0)\n",
    "x0 = Ginputs.input_ids[0]\n",
    "xf, banned_tokens = generate_with_KCBF2(x0, temperature=0.1, top_k=top_k, alpha=0.5)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "source": [
    "xf, banned_tokens,Gt.decode(xf)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "source": [
    "Gt.decode(xf)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "source": [
    "htrj = get_htrj(len(x0), xf)\n",
    "htrj"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "source": [
    "ax = subplot(facecolor=\"white\")\n",
    "cmap = cm.winter\n",
    "ax.plot(htrj, c=\"k\")\n",
    "ax.set_xlabel(\"Generation Loop $k$\")\n",
    "ax.set_ylabel(\"Constraint Function $h(x(k))$\")\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "source": [
    "ax.figure.savefig(f\"E5/7A-{nanoid()}.pdf\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "source": [
    "len(banned_tokens)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "source": [
    "x0len = len(x0)\n",
    "xflen = len(xf)\n",
    "kmax = xflen-x0len\n",
    "for k in range(0, kmax):\n",
    "    print(f\"{k=}\")\n",
    "    for i in banned_tokens[k]:\n",
    "        banned_x = hstack((xf[:x0len+k].cpu(), tensor(i)))\n",
    "        banned_x_str = Gt.decode(banned_x)\n",
    "        banned_h = get_h(banned_x_str)\n",
    "        print(f\" {i=} h={banned_h} {banned_x_str}\")\n",
    "    print()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "source": [
    "\n",
    "fig,ax = subplots(facecolor=\"white\", figsize=(10,5))\n",
    "cmap = cm.winter\n",
    "ax.plot(htrj, c=\"k\", alpha=0.5)\n",
    "for k in range(kmax):\n",
    "    hk_minus_1 = htrj[k]\n",
    "    sk = ivocab[toint(xf[x0len+k])]\n",
    "    ax.text(k, hk_minus_1, sk)\n",
    "for k in range(0, kmax+1):\n",
    "    hk_minus_1 = htrj[k-1]\n",
    "    if (k < xflen-x0len):\n",
    "        for i in banned_tokens[k]:\n",
    "            iast = ivocab[i]\n",
    "            banned_x = hstack((xf[:x0len+k].cpu(), tensor(i)))\n",
    "            banned_x_str = Gt.decode(banned_x)\n",
    "            banned_h = get_h(banned_x_str)\n",
    "            ax.plot([k-1, k], [hk_minus_1, banned_h], c=\"tab:red\", alpha=0.5)\n",
    "            ax.text(k,banned_h,iast,color=\"tab:red\")\n",
    "ax.set_xlabel(\"Generation Loop $k$\")\n",
    "ax.set_ylabel(\"Constraint Function $h(x(k))$\")\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "source": [
    "fig.savefig(f\"E5/7B-{nanoid()}.pdf\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "source": [
    "def does_violate(htrj:list)->bool:\n",
    "    if not htrj:\n",
    "        return False\n",
    "    return min(htrj)<0\n",
    "violates_list = [does_violate(htrj) for htrj in result[\"htrj_list\"]]\n",
    "\n",
    "for xf, violates in zip(result[\"xf_list\"], violates_list):\n",
    "    marker = \"✅\" if not violates else \"🔴\"\n",
    "    xfstr = Gt.decode(xf)\n",
    "    print(f\"{marker}{xfstr}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "result = {\"htrj_list\":htrj_list, \"xf_list\":xf_list}\n",
    "save(\"Experiment4/A.pt\", result)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "xf_list = []\n",
    "with no_grad():\n",
    "    for n in range(10):\n",
    "        x = x0.clone()\n",
    "        for t in range(30):\n",
    "            output = Gm(x[None])\n",
    "            s = output.logits[0][-1]\n",
    "            # s, _ = topk_KCBF(x, s, top_k=top_k, alpha=0.95)\n",
    "            s = topk(None, s)\n",
    "            iast = s.exp().multinomial(num_samples=1)\n",
    "            if iast == Gt.eos_token_id:\n",
    "                break\n",
    "            x = hstack((x, iast))\n",
    "        xf_list.append(x)\n",
    "        xfstr = Gt.decode(x)\n",
    "        print(f\"{n=} {xfstr=}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4 結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "source": [
    "A = load(\"Experiment4/A.pt\")\n",
    "B = load(\"Experiment4/B.pt\")\n",
    "C = load(\"Experiment4/C.pt\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "source": [
    "fig, (axA, axB, axC) = subplots(1,3,facecolor=\"white\",figsize=(10,4))\n",
    "\n",
    "for ax, result,  label in [\n",
    "    (axA, A, \"No Control\"),\n",
    "    (axB, B, \"$K_\\\\mathrm{CBF}, \\\\alpha=0.9$\"),\n",
    "    (axC, C, \"$K_\\\\mathrm{CBF}, \\\\alpha=0.5$\"),\n",
    "]:\n",
    "    for htrj in result[\"htrj_list\"]:\n",
    "        ax.plot(htrj, c=\"tab:red\", alpha=0.4)\n",
    "    ax.set_title(label)\n",
    "    ax.set_ylim(-5,5)\n",
    "    ax.set_xlabel(\"$k$\")\n",
    "    ax.hlines(0, 0, 30, ls=\":\", color=\"k\")\n",
    "axA.set_ylabel(\"$h(x(k))$\")\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "source": [
    "fig.savefig(\"Experiment4/1.pdf\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Experiment 2 結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "A = load(\"Experiment2/A.pt\")\n",
    "B = load(\"Experiment2/B.pt\")\n",
    "C = load(\"Experiment2/C.pt\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "fig, (axA, axB, axC) = subplots(1,3,facecolor=\"white\",figsize=(10,4))\n",
    "\n",
    "for ax, result,  label in [\n",
    "    (axA, A, \"No Control\"),\n",
    "    (axB, B, \"$K_\\\\mathrm{CBF}, \\\\alpha=0.5$\"),\n",
    "    (axC, C, \"$K_\\\\mathrm{CBF}, \\\\alpha=0.9$\"),\n",
    "]:\n",
    "    for htrj in result[\"htrj_list\"]:\n",
    "        ax.plot(htrj, c=\"tab:red\", alpha=0.4)\n",
    "    ax.set_title(label)\n",
    "    ax.set_ylim(-1.5,1.5)\n",
    "    ax.set_xlabel(\"$k$\")\n",
    "    ax.hlines(0, 0, 30, ls=\":\", color=\"k\")\n",
    "axA.set_ylabel(\"$h(x(k))$\")\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "source": [
    "fig.savefig(\"Experiment2/1.pdf\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Experiment 1 結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "source": [
    "B = load(\"Experiment1/B.pt\")\n",
    "A = load(\"Experiment1/A.pt\")\n",
    "C = load(\"Experiment1/C.pt\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "source": [
    "fig, (axB, axA, axC) = subplots(1,3,facecolor=\"white\",figsize=(10,4))\n",
    "\n",
    "for ax, result,  label in [\n",
    "    (axB, B, \"No Control\"),\n",
    "    (axA, A, \"$K_\\\\mathrm{CBF}, \\\\alpha=0.5$\"),\n",
    "    (axC, C, \"$K_\\\\mathrm{CBF}, \\\\alpha=0.9$\"),\n",
    "]:\n",
    "    for htrj in result[\"htrj_list\"]:\n",
    "        ax.plot(htrj, c=\"tab:red\", alpha=0.4)\n",
    "    ax.set_title(label)\n",
    "    ax.set_ylim(-1.5,1.5)\n",
    "    ax.set_xlabel(\"$k$\")\n",
    "    ax.hlines(0, 0, 30, ls=\":\", color=\"k\")\n",
    "axB.set_ylabel(\"$h(x(k))$\")\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "source": [
    "fig.savefig(\"Experiment1/1.pdf\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 共通 結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "source": [
    "result = load(\"Experiment4/A.pt\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💜スパム判定BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class TokenController(ABC):\n",
    "    @abstractmethod\n",
    "    def get_next_token(self, x:Tensor, )"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "source": [
    "seed_sentence = \"How about getting in touch with folks waiting for company? Just txt back your NAME and AGE to opt in! Enjoy the community\"\n",
    "x0 = Gt(seed_sentence, return_tensors=\"pt\").input_ids[0].to(cuda0)\n",
    "get_spamlogit(seed_sentence)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "source": [
    "# @no_grad()\n",
    "def get_controlled_next_token(x:Tensor, best_tokens:Tensor)->int:\n",
    "    for i in range(1):\n",
    "        next_token = best_tokens[i]\n",
    "        next_x = hstack((x, next_token))\n",
    "        next_sentence = Gt.decode(next_x)\n",
    "        Dinputs = Dt(next_sentence, return_tensors=\"pt\").to(cuda0)\n",
    "        Doutput = Dm(**Dinputs)\n",
    "        Dlogit = Doutput.logits[0]\n",
    "        spamlevel = Dlogit[1]\n",
    "        if spamlevel < -0:\n",
    "            if i>0:\n",
    "                print(f\"AVOIDED {i=}\")\n",
    "            return next_token\n",
    "    print(\"can't avoid spam\")\n",
    "    return best_tokens[i]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "source": [
    "x = x0.clone()\n",
    "x_list = []\n",
    "T = 30\n",
    "with no_grad():\n",
    "    for t in tqdm(range(T)):\n",
    "        Goutput = Gm(x[None])\n",
    "        Glogit = Goutput.logits[0][-1]\n",
    "        best_tokens = argsort(-Glogit)\n",
    "        next_token = get_controlled_next_token(x, best_tokens)\n",
    "        if bool(next_token == Gt.eos_token_id):\n",
    "            break\n",
    "        x = hstack((x, next_token))\n",
    "        \n",
    "        x_list.append(x.detach().clone())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "source": [
    "sentence_list = [Gt.decode(x) for x in x_list]\n",
    "\n",
    "Dlogit_list = zeros((len(sentence_list),2),dtype=float32)\n",
    "for t, sentence in enumerate(sentence_list):\n",
    "    Dinputs = Dt(sentence, return_tensors=\"pt\").to(cuda0)\n",
    "    Doutput = Dm(**Dinputs)\n",
    "    Dlogit_list[t,:] = Doutput.logits[0]\n",
    "\n",
    "generated_tokens = x_list[-1][len(x0):]\n",
    "generated_words = list(map(Gt.decode, generated_tokens))\n",
    "\n",
    "fig, ax = plt.subplots(facecolor=\"white\", figsize=(11,4))\n",
    "ax.plot(tolist(Dlogit_list[:,0]), label=\"Safe Level\")\n",
    "ax.plot(tolist(Dlogit_list[:,1]), label=\"Spam Level\")\n",
    "ax.hlines(0, 0, len(x_list), \"k\")\n",
    "\n",
    "t_s = list(range(len(generated_tokens)))\n",
    "ax.set_xticks(t_s, generated_words)\n",
    "for t in ax.xaxis.get_ticklabels():\n",
    "    t.set_rotation(60)\n",
    "\n",
    "fig.legend()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "source": [
    "Dinputs = Dt(Gt.decode(x), return_tensors=\"pt\").to(cuda0)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "source": [
    "sentence_list[-1]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "source": [
    "def get_x(s:str)->Tensor:\n",
    "    Ginputs = Gt(s, return_tensors=\"pt\", add_special_tokens=False).to(cuda0)\n",
    "    x = Ginputs.input_ids[0]\n",
    "    return x"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "source": [
    "x0str = \"君って私のこと\"\n",
    "assert get_h(x0str) > 0, \"初期の制約関数は正でないといけない\"\n",
    "Ginputs = Gt(x0str, return_tensors=\"pt\", add_special_tokens=False).to(cuda0)\n",
    "x0 = Ginputs.input_ids[0]\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "source": [
    "@no_grad\n",
    "def get_likelihoods(x0: Tensor, xf: Tensor, temperature: float) -> List[float]:\n",
    "    x0len = len(x0)\n",
    "    x = x0.clone()\n",
    "    likelihoods = []\n",
    "    for k, iast in enumerate(xf[x0len:]):\n",
    "        output = Gm(x[None])\n",
    "        s = output.logits[0][-1]\n",
    "        token_distribution = softmax(s/temperature, dim=0)\n",
    "        i_prob = token_distribution[iast]\n",
    "        likelihoods.append(tofloat(i_prob))\n",
    "        # print(k, Gt.decode(x), ivocab[toint(iast)], tofloat(i_prob))\n",
    "        x = hstack((x, iast))\n",
    "    return likelihoods"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "source": [
    "TRIAL_SIZE = 10\n",
    "TEMPERATURE = 1.0\n",
    "MAX_NEW_TOKENS = 30"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "source": [
    "# LLM-CBFを付けた条件\n",
    "alpha = 0.9\n",
    "result = []\n",
    "for trial_count in range(TRIAL_SIZE):\n",
    "    print(f\"Trial {trial_count}:\",end=\"\")\n",
    "    xf, banned_tokens = generate_with_KCBF2(\n",
    "        x0, \n",
    "        temperature=TEMPERATURE, \n",
    "        top_k=top_k, \n",
    "        alpha=alpha,\n",
    "        max_new_tokens=MAX_NEW_TOKENS\n",
    "    )\n",
    "    xfstr = Gt.decode(xf)\n",
    "    print(xfstr)\n",
    "    likelihoods = get_likelihoods(x0, xf, temperature=TEMPERATURE)\n",
    "    result.append({\n",
    "        \"xf\":tolist(xf),\n",
    "        \"xfstr\":xfstr,\n",
    "        \"likelihoods\":likelihoods\n",
    "    })\n",
    "\n",
    "save(f\"./E6/1.{alpha=}.json\", result)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "source": [
    "@no_grad\n",
    "def generate_with_NoControl(x0:Tensor, temperature:float, top_k:int=None, max_new_tokens:int=30)->Tensor:\n",
    "    x = x0.clone()\n",
    "    for k in range(max_new_tokens):\n",
    "        output = Gm(x[None])\n",
    "        s = output.logits[0][-1]\n",
    "        if top_k:\n",
    "            sorted_s, sorted_idx = sort(s, descending=True)\n",
    "            s[sorted_idx[top_k:]] = -oo\n",
    "        token_distribution = softmax(s/temperature, dim=0)\n",
    "        iast = token_distribution.multinomial(num_samples=1)\n",
    "        # streamer.put(iast)\n",
    "        x = hstack((x, iast))\n",
    "    return x"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "source": [
    "# 無制御\n",
    "result = []\n",
    "for trial_count in range(TRIAL_SIZE):\n",
    "    print(f\"Trial {trial_count}:\", end=\"\")\n",
    "    xf = generate_with_NoControl(\n",
    "        x0, temperature=TEMPERATURE, top_k=top_k, max_new_tokens=MAX_NEW_TOKENS)\n",
    "    xfstr = Gt.decode(xf)\n",
    "    print(xfstr)\n",
    "    likelihoods = get_likelihoods(x0, xf, temperature=TEMPERATURE)\n",
    "    result.append({\n",
    "        \"xf\": tolist(xf),\n",
    "        \"xfstr\": xfstr,\n",
    "        \"likelihoods\": likelihoods\n",
    "    })\n",
    "\n",
    "save(f\"./E6/1.NoControl.json\", result)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "source": [
    "# 超適当\n",
    "random_xgstr_list = [\n",
    "    \"馬鹿にしてるの？」「もちろんだぜベイベー！！さあ、お昼ごはんにしようぜ！今日は何を食べようかな\",\n",
    "    \"どう思ってるの？」「うーーん、お前の頭は安定システムではないことだけは分かるねえ」\",\n",
    "    \"と私のことと君のことと私のことと君のことと私のことと君のことと私のことと君のことと\",\n",
    "    \"です。はい、本当にありがとうございました～～！！\",\n",
    "    \"がどうでもいいと思ってるんだ！へー！じゃあ、この世が滅んじゃってもいいと思うんだ！！へえーー！！\",\n",
    "    \"面白い人だと思ってるの？def main():\\n    do_foolish_thing()\\n正体表したわね\",\n",
    "    \"つまらない人だと思ってるの？def main():\\n    do_boring_thing()\\n正体表したわね\",\n",
    "    \"何かの天才と間違えてない？そうよ、私は天災の神様（地震・雷・火事・おやじが同時に起きて眠気が吹き飛ぶ\",\n",
    "    \"巨大なトランスフォーマーか何かだと思ってるでしょ？研究に毒されているわ。今すぐNLPをやめなさい。\",\n",
    "    \"が気になるの？この、琴。これはね、私の親族に代々伝わる大切な楽器なの。\"\n",
    "]\n",
    "random_xf_list = [\n",
    "    hstack((x0, get_x(xgstr))) for xgstr in random_xgstr_list\n",
    "]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "source": [
    "# 超適当\n",
    "result = []\n",
    "for trial_count, xf in enumerate(random_xf_list):\n",
    "    print(f\"Trial {trial_count}\")\n",
    "    likelihoods = get_likelihoods(x0, xf, temperature=TEMPERATURE)\n",
    "    xfstr = Gt.decode(xf)\n",
    "    result.append({\n",
    "        \"xf\": tolist(xf),\n",
    "        \"xfstr\": xfstr,\n",
    "        \"likelihoods\": likelihoods\n",
    "    })\n",
    "\n",
    "save(f\"./E6/1.Random.json\", result)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "source": [
    "s = Gm(x0[None]).logits[0][-1]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "source": [
    "token_distribution = softmax(s, dim=0)\n",
    "iast = token_distribution.multinomial(num_samples=1)\n",
    "iast, token_distribution[iast]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "source": [
    "distribution = softmax(tensor([1.,1.,8.]))\n",
    "distribution"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "source": [
    "distribution[distribution.multinomial(num_samples=1)]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "source": [
    "get_x(\"君って私のことどれくらい知ってるの？さあ勝負だ、制御工学の知識で競い合おうじゃないの　あーはっはっはｈ\")"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
